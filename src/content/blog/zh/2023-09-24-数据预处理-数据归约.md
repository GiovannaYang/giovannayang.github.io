---
title: 数据预处理-数据归约
tags:
  - 数据挖掘
---

用来得到数据集的归约表示，它小得多，但可以产生相同的(或几乎相同的)分析结果。

**数据归约策略**：

- 维归约：小波分析、PCA、特征筛选
- 数量归约：直方图、回归、聚类、抽样、数据立方体聚集
- 数据压缩：使用变换

## 一、维归约

### 1. 小波变换

![](/img/posts/zh/2023-09-24/020901.png)

### 2. PCA

基本思想：找到一个投影，其能表示数据的最大变化

![](/img/posts/zh/2023-09-24/020902.png)

### 3. 特征筛选

目的：通过删除不相干的属性或维减少数据量

挑战：d个属性有2d个可能的子集 (枚举所有几乎不可行)

策略：启发式的方法

- 逐步向前选择
- 逐步向后删除
- 向前选择和向后删除相结合

算法：

- 信息增益(Information Gain)
- ID3
- 互信息(Mutual Information)
- Relief
- 卡方分析

## 补充：信息增益

**信息熵**：刻画系统的混乱程度

![](/img/posts/zh/2023-09-24/020903.png)

定义：

$$
H(X)=-\sum_{i=1}^{n}p(x_i)\log p(x_i)
$$
 
例子：

![](/img/posts/zh/2023-09-24/020904.png)

**条件信息熵**：刻画在已知X的基础上需要多少信息来描述Y

定义：

$$
H(Y|X)\equiv\sum_{x\in\Chi}p(x)H(Y|X=x)=-\sum_{x\in\Chi}p(x)\sum_{y\in\Upsilon}p(y|x)\log p(y|x)
$$

例子：

![](/img/posts/zh/2023-09-24/020905.png)

**信息增益**：刻画在已知X的基础上需要节约多少信息来描述Y

定义：$IG(Y|X)=H(Y)-H(Y|X)$

**信息增益与特征筛选**

  - 基本思想：选择那些特征对分类变量Y信息增益大，删除那些对分类无用的特征。

例子：

![](/img/posts/zh/2023-09-24/020906.png)

## 二、数量归约

基本思想：通过选择替代的、较小的数据表示形式来减少数据量。

### 1. 有参方法

使用一个参数模型估计数据，最后只要存储参数即可。

- 线性回归方法：Y=α+βX

- 多元回归：线性回归的扩充

- 对数线性模型：近似离散的多维数据概率分布

### 2. 无参方法

- 直方图

- 聚类

- 抽样

- 数据立方体聚集

#### 直方图

一种流行的数据归约技术，将某属性的数据划分为不相交的子集或桶，桶中放置该值的出现频率。

桶和属性值的划分规则

  - 等宽
  - 等深
  - V-最优
  - MaxDiff

#### 聚类

将数据集划分为聚类，然后通过聚类来表示数据集。

如果数据可以组成各种不同的聚类，则该技术非常有效，反之如果数据界线模糊，则方法无效。

数据可以分层聚类，并被存储在多层索引树中。

#### 抽样

允许用数据的较小随机样本(子集)表示大的数据集。

对数据集D的样本选择：

  - 简单随机选择n个样本，不回放：由D的N个元组中抽取n个样本
  - 简单随机选择n个样本，回放：过程同上，只是元组被抽取后，将被回放，可能再次被抽取
  - 聚类选样：D中元组被分入M个互不相交的聚类中，可在其中的m个聚类上进行简单随机选择(m＜M)
  - 分层选样：D被划分为互不相交的“层”，则可通过对每一层的简单随机选样得到D的分层选样

#### 数据立方体聚集

- 最底层的方体对应于基本方体

  - 基本方体对应于感兴趣的实体

- 数据立方体可以看成方体的格

  - 在数据立方体中存在着不同级别的汇总
  - 每个较高层次的抽象将进一步减少结果数据

- 数据立方体提供了对预计算的汇总数据的快速访问

  - 使用与给定任务相关的最小方体
  - 在可能的情况下，对于汇总数据的查询应当使用数据立方体

## 三、数据压缩

- 有损压缩 VS. 无损压缩

- 字符串压缩

  - 有广泛的理论基础和精妙的算法
  - 通常是无损压缩
  - 在解压缩前对字符串的操作非常有限

- 音频/视频压缩

  - 通常是有损压缩，压缩精度可以递进选择
  - 有时可以在不解压整体数据的情况下，重构某个片断
